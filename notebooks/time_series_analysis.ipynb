{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7e2ca7-e7c1-44af-a6ed-0bbfac11553f",
   "metadata": {},
   "source": [
    "# Time Series Analysis with PySpark Window Functions\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates PySpark window functions for time series analysis: \n",
    "- **lag()**: Access previous row values\n",
    "- **lead()**: Access next row values  \n",
    "- **Moving averages**: Calculate rolling statistics\n",
    "\n",
    "## Use Case\n",
    "Analyzing traffic patterns over time to identify trends and anomalies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468f37dd-bf22-4fea-84a1-cd9a23ad9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1261ae4a-6a49-49ce-b66f-83043c041339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.14 (main, Oct  9 2025, 16:16:55) [Clang 17.0.0 (clang-1700.4.4.1)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5fde1c-e308-4b7c-82d4-62c105cd9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/07 16:07:23 WARN Utils: Your hostname, Zipcoders-MacBook-Pro-8.local resolves to a loopback address: 127.0.0.1; using 192.168.200.39 instead (on interface en0)\n",
      "26/01/07 16:07:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/07 16:07:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized!\n",
      "Spark version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TimeSeriesAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "409789ef-45b1-4f88-8b54-7d6d7e4da37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data created:\n",
      "+----------+-------------------+-------------+\n",
      "| sensor_id|          timestamp|vehicle_count|\n",
      "+----------+-------------------+-------------+\n",
      "|sensor_001|2024-01-01 08:00:00|           45|\n",
      "|sensor_001|2024-01-01 08:05:00|           52|\n",
      "|sensor_001|2024-01-01 08:10:00|           48|\n",
      "|sensor_001|2024-01-01 08:15:00|           55|\n",
      "|sensor_002|2024-01-01 08:00:00|           30|\n",
      "|sensor_002|2024-01-01 08:05:00|           35|\n",
      "+----------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"timestamp\")\n",
    "\n",
    "# Let's create some sample data first to test\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create sample traffic data\n",
    "sample_data = [\n",
    "    (\"sensor_001\", datetime(2024, 1, 1, 8, 0), 45),\n",
    "    (\"sensor_001\", datetime(2024, 1, 1, 8, 5), 52),\n",
    "    (\"sensor_001\", datetime(2024, 1, 1, 8, 10), 48),\n",
    "    (\"sensor_001\", datetime(2024, 1, 1, 8, 15), 55),\n",
    "    (\"sensor_002\", datetime(2024, 1, 1, 8, 0), 30),\n",
    "    (\"sensor_002\", datetime(2024, 1, 1, 8, 5), 35),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"vehicle_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b7f7d7-6077-4dc6-aebc-c5b796c1d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LAG function (previous value):\n",
      "+----------+-------------------+-------------+--------------+\n",
      "| sensor_id|          timestamp|vehicle_count|previous_count|\n",
      "+----------+-------------------+-------------+--------------+\n",
      "|sensor_001|2024-01-01 08:00:00|           45|          null|\n",
      "|sensor_001|2024-01-01 08:05:00|           52|            45|\n",
      "|sensor_001|2024-01-01 08:10:00|           48|            52|\n",
      "|sensor_001|2024-01-01 08:15:00|           55|            48|\n",
      "|sensor_002|2024-01-01 08:00:00|           30|          null|\n",
      "|sensor_002|2024-01-01 08:05:00|           35|            30|\n",
      "+----------+-------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test LAG - get previous value\n",
    "df_with_lag = df.withColumn(\n",
    "    \"previous_count\", \n",
    "    F.lag(\"vehicle_count\", 1).over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"Testing LAG function (previous value):\")\n",
    "df_with_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a109b47-ece3-499a-bc19-dd2fcec6cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LEAD function (next value):\n",
      "+----------+-------------------+-------------+----------+\n",
      "| sensor_id|          timestamp|vehicle_count|next_count|\n",
      "+----------+-------------------+-------------+----------+\n",
      "|sensor_001|2024-01-01 08:00:00|           45|        52|\n",
      "|sensor_001|2024-01-01 08:05:00|           52|        48|\n",
      "|sensor_001|2024-01-01 08:10:00|           48|        55|\n",
      "|sensor_001|2024-01-01 08:15:00|           55|      null|\n",
      "|sensor_002|2024-01-01 08:00:00|           30|        35|\n",
      "|sensor_002|2024-01-01 08:05:00|           35|      null|\n",
      "+----------+-------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test LEAD - get next value\n",
    "df_with_lead = df.withColumn(\n",
    "    \"next_count\", \n",
    "    F.lead(\"vehicle_count\", 1).over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"Testing LEAD function (next value):\")\n",
    "df_with_lead.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b12288b-e5fa-4f1a-ab89-e9120bcfe4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MOVING AVERAGE (3-point window):\n",
      "+----------+-------------------+-------------+------------------+\n",
      "| sensor_id|          timestamp|vehicle_count|      moving_avg_3|\n",
      "+----------+-------------------+-------------+------------------+\n",
      "|sensor_001|2024-01-01 08:00:00|           45|              45.0|\n",
      "|sensor_001|2024-01-01 08:05:00|           52|              48.5|\n",
      "|sensor_001|2024-01-01 08:10:00|           48|48.333333333333336|\n",
      "|sensor_001|2024-01-01 08:15:00|           55|51.666666666666664|\n",
      "|sensor_002|2024-01-01 08:00:00|           30|              30.0|\n",
      "|sensor_002|2024-01-01 08:05:00|           35|              32.5|\n",
      "+----------+-------------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test MOVING AVERAGE - 3-point rolling average\n",
    "rolling_window = Window.partitionBy(\"sensor_id\").orderBy(\"timestamp\").rowsBetween(-2, 0)\n",
    "\n",
    "df_with_avg = df.withColumn(\n",
    "    \"moving_avg_3\", \n",
    "    F.avg(\"vehicle_count\").over(rolling_window)\n",
    ")\n",
    "\n",
    "print(\"Testing MOVING AVERAGE (3-point window):\")\n",
    "df_with_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e9cc2-4314-47a9-a362-4639487e50bc",
   "metadata": {},
   "source": [
    "## Window Functions Tested:\n",
    "\n",
    "1. **LAG**: Gets the previous value in the time series\n",
    "   - Useful for: Comparing current vs previous readings\n",
    "   \n",
    "2. **LEAD**: Gets the next value in the time series\n",
    "   - Useful for: Looking ahead in predictions\n",
    "   \n",
    "3. **MOVING AVERAGE**: Calculates average over a rolling window\n",
    "   - Useful for: Smoothing out noise in sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bef5f7-3b40-445e-ad77-e81059a38bd2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Apply to Real Sample Data\n",
    "\n",
    "Now let's load Alan's actual traffic sensor sample data and apply the same window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710b45d6-da7f-4501-8a22-054bac80c3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Alan's sample traffic data:\n",
      "Total rows: 500\n",
      "Columns: ['sensor_id', 'timestamp', 'location_lat', 'location_lon', 'vehicle_count', 'avg_speed', 'congestion_level', 'road_type']\n",
      "+-----------+--------------------+------------------+------------------+-------------+---------+----------------+-----------+\n",
      "|  sensor_id|           timestamp|      location_lat|      location_lon|vehicle_count|avg_speed|congestion_level|  road_type|\n",
      "+-----------+--------------------+------------------+------------------+-------------+---------+----------------+-----------+\n",
      "|TRAFFIC_003|2026-01-04 01:39:...|39.994546889470804|-75.15878485055464|           50|    49.22|        Critical|Main Street|\n",
      "|TRAFFIC_006|2026-01-05 00:39:...| 39.94164075651462|-75.14205586940092|          406|    62.87|        Critical|Main Street|\n",
      "|TRAFFIC_000|2026-01-02 19:39:...| 39.92581673937803|-75.11199193892674|          418|     60.9|            High|    Highway|\n",
      "|TRAFFIC_003|2026-01-01 11:39:...| 39.94685511939174| -75.1583224616334|          356|    19.94|        Critical|     Avenue|\n",
      "|TRAFFIC_005|2026-01-02 03:39:...|39.946665864490534|-75.17589141908685|          393|    67.98|        Critical|    Highway|\n",
      "|TRAFFIC_006|2026-01-04 21:39:...| 39.90500024787361|-75.15355204599955|           49|    31.36|            High|Side Street|\n",
      "|TRAFFIC_004|2026-01-02 23:39:...|39.986597798831674|-75.19212151267357|          478|    16.38|          Medium|     Avenue|\n",
      "|TRAFFIC_003|2026-01-03 00:39:...| 39.94498263315836|-75.20262814776339|          375|    29.06|        Critical|Main Street|\n",
      "|TRAFFIC_006|2026-01-01 17:39:...| 39.95769227692332|-75.19076048295621|          270|    43.54|            High|Main Street|\n",
      "|TRAFFIC_000|2026-01-05 16:39:...| 39.91808414736588|  -75.195401965358|          438|    16.35|        Critical|    Highway|\n",
      "+-----------+--------------------+------------------+------------------+-------------+---------+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Rows with null timestamps: 0\n"
     ]
    }
   ],
   "source": [
    "# Load Alan's sample traffic data (go up one directory first)\n",
    "traffic_sample = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../data/samples/traffic_sample.csv\")\n",
    "\n",
    "# Convert timestamp to proper format\n",
    "traffic_sample = traffic_sample.withColumn(\n",
    "    \"timestamp\", \n",
    "    F.to_timestamp(F.col(\"timestamp\"))\n",
    ")\n",
    "\n",
    "print(\"✅ Loaded Alan's sample traffic data:\")\n",
    "print(f\"Total rows: {traffic_sample.count()}\")\n",
    "print(f\"Columns: {traffic_sample.columns}\")\n",
    "traffic_sample.show(10)\n",
    "\n",
    "# Check for any null timestamps\n",
    "null_count = traffic_sample.filter(F.col(\"timestamp\").isNull()).count()\n",
    "print(f\"Rows with null timestamps: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1fa4b-1f5f-460d-be31-9f5c28fc3238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
